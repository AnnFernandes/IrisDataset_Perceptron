{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_452.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQRSRdnnY0O",
        "colab_type": "text"
      },
      "source": [
        "# CMPE 452 - ASSIGNMENT 1\n",
        "\n",
        "Perceptron scratch algoithim and the tensorflow keras is included in this file. Please run the first block for each of the algorithims to get the training and test files imported into the code. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKPpUet7jO2X",
        "colab_type": "code",
        "outputId": "f6fb6090-af6d-45c9-e9aa-881be4bfd373",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# Import statements - Run for both keras and python based code\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import os\n",
        "import math as math\n",
        "np.genfromtxt\n",
        "\n",
        "#on your computer choose the files  to train and test - make sure that they have names iris_test.txt, iris_train.txt (Select both of them at the same time)\n",
        "from google.colab import files\n",
        "upload = files.upload()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8bef0e01-b5e1-4d8f-8393-632be3fa88b0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8bef0e01-b5e1-4d8f-8393-632be3fa88b0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris_test.txt to iris_test.txt\n",
            "Saving iris_train.txt to iris_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "902snRfmCRkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Function used to split the data from inputs and outputs\n",
        "def importDataAndSplit():\n",
        "  #Splitting data\n",
        "  irisTestX = genfromtxt('iris_test.txt', delimiter= \",\", usecols=[0,1,2,3]) #take the first 4 columns for input for testing\n",
        "  irisTestY = genfromtxt('iris_test.txt', delimiter= \",\", usecols=[4], dtype=str) # take last column for output for testing\n",
        "\n",
        "  irisTrainX = genfromtxt('iris_train.txt', delimiter= \",\", usecols=[0,1,2,3]) #take the first 4 columns for input for training\n",
        "  irisTrainY = genfromtxt('iris_train.txt', delimiter= \",\", usecols=[4], dtype=str) # take last column for output for training\n",
        "  \n",
        "  #Add a -l column at the end of each dataset as a bias input for both training and testing\n",
        "  negTrain = np.ones((irisTrainX.shape[0],1))\n",
        "  negTrain = negTrain*-1\n",
        "  negTest = np.ones((irisTestX.shape[0],1))\n",
        "  negTest = negTest*-1\n",
        "  irisTrainX = np.concatenate( [ irisTrainX, negTrain ] , axis = 1)\n",
        "  irisTestX = np.concatenate( [ irisTestX, negTest ] , axis = 1)\n",
        "  \n",
        "  \n",
        "  #converting from string to int for the classification of the flowers for both training and testing\n",
        "  # 0 - Iris-setosa\n",
        "  # 1 - Iris-versicolor\n",
        "  # 2 - Iris-virgina\n",
        "  for i in range(irisTrainY.shape[0]):\n",
        "    if irisTrainY[i] == \"Iris-setosa\":\n",
        "      irisTrainY[i] = \"0\"\n",
        "    elif irisTrainY[i] == \"Iris-versicolor\":\n",
        "      irisTrainY[i] = \"1\"\n",
        "    else:\n",
        "      irisTrainY[i] = \"2\"\n",
        "\n",
        "  for i in range(irisTestY.shape[0]):\n",
        "    if irisTestY[i] == \"Iris-setosa\":\n",
        "      irisTestY[i] = \"0\"\n",
        "    elif irisTestY[i] == \"Iris-versicolor\":\n",
        "      irisTestY[i] = \"1\"\n",
        "    else:\n",
        "      irisTestY[i] = \"2\"\n",
        "\n",
        "  irisTrainY = irisTrainY.astype(np.int)\n",
        "  irisTestY = irisTestY.astype(np.int)\n",
        "  \n",
        "  #return the 4 data sets \n",
        "  return irisTestX, irisTestY, irisTrainX, irisTrainY\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV-XIQE6ly1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "\n",
        "    #define the attirubutes for the function\n",
        "    def __init__(self, no_of_inputs, itera=1000, c=0.01):\n",
        "        self.itera = itera #interation\n",
        "        self.c = c #learning rate\n",
        "        self.weights = np.random.rand(no_of_inputs + 1, 3) #weights as a 2D array (15)\n",
        "        self.longestrun = 0 #for the pocket alogorithim - store the longest run\n",
        "        self.currentrun = 0 #for the pocket algorithim - store the current run\n",
        "        self.best_weights =  np.copy(self.weights) #for the pocket algorithim - to put the best found weights\n",
        "        \n",
        "    #Used to predict which perceptron/neuron has fired - takes the inputs to find output\n",
        "    def predict(self, inputs):\n",
        "        \n",
        "        #take the dot product which muliples the weights with associated inputs \n",
        "        summation = np.dot(inputs,self.best_weights)#returns a 1*3 matric\n",
        "        max1 = -100\n",
        "        spot = -1\n",
        "        #find the highest value and return the location\n",
        "        for r in range(3):\n",
        "          if summation[r] > max1:\n",
        "            max1 = summation[r]\n",
        "            spot = r\n",
        "        return spot \n",
        "      \n",
        "    #Used to train the network - takes the training input and output\n",
        "    def train(self, training_inputs, training_outputs):\n",
        "        #for the amount of iterations we want to \n",
        "        for k in range(self.itera):\n",
        "            error = 0\n",
        "            #for all the data sets in training \n",
        "            for inputs, output in zip(training_inputs, training_outputs):\n",
        "                \n",
        "                #forward propogation\n",
        "                z2 = np.dot(inputs,self.weights)\n",
        "                #apply sigmoid function so between 1 and 0\n",
        "                a2 = sigmoid(z2) #1*3 matrix\n",
        "                  \n",
        "                #find which neuron of the 3 should be activated\n",
        "                target = np.zeros(3)\n",
        "                target[output] = 1\n",
        "                \n",
        "                \n",
        "                for i in range(3):\n",
        "                  error += (target[i] - a2[i]) ** 2\n",
        "                \n",
        "                #find the error using the target - predicated value from the sigmoid function\n",
        "                change = []\n",
        "                for i in range(3):\n",
        "                  x = (target[i] - a2[i])\n",
        "                  change.append(x)\n",
        "                \n",
        "                if np.argmax(a2) == np.argmax(target): #if equal increase run length\n",
        "                    self.currentrun += 1\n",
        "                    if self.currentrun > self.longestrun:   #if longestrun is less than or equal to current run\n",
        "                      self.longestrun = self.currentrun\n",
        "                      self.best_weights = np.copy(self.weights) #copy current weights into best weights\n",
        "                else:\n",
        "                    self.currentrun = 0 #set curent run to 0 and apply weight changes\n",
        "                    for y in range(4):\n",
        "                      for x in range(3):\n",
        "                        self.weights[y][x] += self.c * (change[x] * inputs[y]) \n",
        "              \n",
        "            if error == 0:\n",
        "               self.best_weights = np.copy(self.weights)\n",
        "               break\n",
        "             \n",
        "    # Used to test the neural network - accepts the x or input values of the test data\n",
        "    def test(self, test_input):\n",
        "        p = []\n",
        "        print (self.best_weights)\n",
        "        #for every data set\n",
        "        for inputs in test_input:\n",
        "          #call the perdict function to find which flower it is and append to list\n",
        "          predication = self.predict(inputs)\n",
        "          p.append(predication)\n",
        "        return p #returns a list of predicated values\n",
        "    \n",
        "                           \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTFrBSlzcSzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for sigmoid function\n",
        "def sigmoid(inX):\n",
        "    r = 1.0/(1+ np.exp(-inX))\n",
        "    return r\n",
        "\n",
        "#Function to output data into a test file\n",
        "def output_data_txt(inputs, predictions):\n",
        "    #puts outputd file on google drive - need to give it access\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    \n",
        "    #Cange the int classification of flowers back to string\n",
        "    pred = np.array(predictions)\n",
        "    p = [[0 for x in range(30)] for y in range(1)]\n",
        "    for i in range (30):\n",
        "      if pred[i] == 0.0:\n",
        "        p[0][i] = \"Iris-setosa\"\n",
        "      elif pred[i] == 1.0:\n",
        "        p[0][i] = \"Iris-versicolor\"\n",
        "      else:\n",
        "        p[0][i] = \"Iris-virginica\" \n",
        "    p = np.array(p, dtype=str)\n",
        "    \n",
        "    #shape the array so now it it vertical\n",
        "    p = np.reshape(p, (30, 1))\n",
        "    #delete the bais input column\n",
        "    inputs = np.delete(inputs, 4, 1) #remove -1 (bias input) column\n",
        "    #combine the 2 arrays together - inputs and predictions\n",
        "    final =  np.concatenate( [ inputs, p ] , axis = 1)\n",
        "    #save to a text file\n",
        "    with open('/content/gdrive/My Drive/Output/output.txt', 'w') as f:\n",
        "      np.savetxt(f, (final), fmt = '%s')\n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqNAF315xC4T",
        "colab_type": "code",
        "outputId": "bbbaacf4-c886-4221-a518-bf71ce5cd01b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "source": [
        "#call the perceptron class and init function - paramters: number of inputs to each perceptron (not including bias)\n",
        "per = Perceptron(4)\n",
        "#call function to split train and test data into 4 lists\n",
        "test_x, test_y, train_x, train_y = importDataAndSplit()\n",
        "#train neural network with training data\n",
        "per.train(train_x, train_y)\n",
        "#use test data to see how accurate it is\n",
        "perd = per.test(test_x)\n",
        "\n",
        "# To see how accurate the neural network is comapred to the correct values\n",
        "acc = 0.0\n",
        "for i in range(30):\n",
        "    if perd[i] == int(test_y[i]):\n",
        "        acc += 1\n",
        "print(acc / 30 * 100, \"%\")\n",
        "\n",
        "# Output information into a text file\n",
        "output_data_txt(test_x, perd)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.00173001  0.09418506 -0.85209047]\n",
            " [ 0.92680948  0.1534393  -0.64316556]\n",
            " [-1.38284192 -0.05207456  1.08696319]\n",
            " [ 0.10197962 -0.0886286   1.13190831]\n",
            " [ 0.37731776  0.76604002  0.30366055]]\n",
            "90.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d25497b4f3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Output information into a text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutput_data_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-bab810cba0bc>\u001b[0m in \u001b[0;36moutput_data_txt\u001b[0;34m(inputs, predictions)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#puts outputd file on google drive - need to give it access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#Cange the int classification of flowers back to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendcontrol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Stopped'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI30MDltSNhG",
        "colab_type": "text"
      },
      "source": [
        "Part 2 - Implementing using tensorflow and keras\n",
        "\n",
        "Please run the first bloack to import the lists into the algorithim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LABsx0uz1qis",
        "colab_type": "code",
        "outputId": "fe8d72a6-16d3-4eed-e97b-b47139368008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Import statements - also run the import block found at the top of code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Separate the x and y data for each the training and testing set\n",
        "test_x = genfromtxt('iris_test.txt', delimiter= \",\", usecols=[0,1,2,3]) #take the first 4 columns for input for testing\n",
        "test_y = genfromtxt('iris_test.txt', delimiter= \",\", usecols=[4], dtype=str) # take last column for output for testing\n",
        "\n",
        "train_x = genfromtxt('iris_train.txt', delimiter= \",\", usecols=[0,1,2,3]) #take the first 4 columns for input for training\n",
        "train_y = genfromtxt('iris_train.txt', delimiter= \",\", usecols=[4], dtype=str)\n",
        "\n",
        "#for the y data \n",
        "for i in range(train_y.shape[0]):\n",
        "    if train_y[i] == \"Iris-setosa\":\n",
        "      train_y[i] = \"0\"\n",
        "    elif train_y[i] == \"Iris-versicolor\":\n",
        "      train_y[i] = \"1\"\n",
        "    else:\n",
        "      train_y[i] = \"2\"\n",
        "\n",
        "for i in range(test_y.shape[0]):\n",
        "    if test_y[i] == \"Iris-setosa\":\n",
        "      test_y[i] = \"0\"\n",
        "    elif test_y[i] == \"Iris-versicolor\":\n",
        "      test_y[i] = \"1\"\n",
        "    else:\n",
        "      test_y[i] = \"2\"\n",
        "\n",
        "test_y = test_y.astype(np.int)\n",
        "train_y = train_y.astype(np.int)\n",
        "\n",
        "\"\"\"\n",
        "Training the model\n",
        "\"\"\"\n",
        "#changes it to a binary persentation \n",
        "train_y = to_categorical(train_y) \n",
        "\n",
        "#build model - using a stack layer model - inputs -> [3 hidden nodes] -> 3 outputs\n",
        "m = tf.keras.Sequential()\n",
        "m.add(Dense(3, input_dim=4, activation='sigmoid')) #\t4 inputs connected to a hidden layer with the activation function sigmoid\n",
        "m.add(Dense(3, activation='softmax')) # output layer with an activation of the softmax function so final result is between 1 and 0\n",
        "\n",
        "#define loss and otptimzer functions\n",
        "m.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#train the model\n",
        "m.fit(train_x, train_y, epochs=550, batch_size=32)\n",
        "\n",
        "\"\"\"\n",
        "Testing model\n",
        "\"\"\"\n",
        "#get prediction of test data\n",
        "cl = m.predict_classes(test_x, batch_size=32)\n",
        "\n",
        "#Print predictions and overall accuracy of the netowrk by comparing it to the true output\n",
        "import numpy as np\n",
        "print(\"Prediction :\")\n",
        "print(cl)\n",
        "print(\"Target :\")\n",
        "print(test_y)\n",
        "print (\"Accuracy: \")\n",
        "acc = 0.0\n",
        "for i in range(30):\n",
        "    if cl[i] == int(test_y[i]):\n",
        "        acc += 1\n",
        "print(acc / 30 * 100, \"%\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/550\n",
            "120/120 [==============================] - 0s 651us/sample - loss: 0.2354 - acc: 0.3333\n",
            "Epoch 2/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.2345 - acc: 0.3333\n",
            "Epoch 3/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.2337 - acc: 0.3333\n",
            "Epoch 4/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.2330 - acc: 0.3333\n",
            "Epoch 5/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.2323 - acc: 0.3333\n",
            "Epoch 6/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.2315 - acc: 0.3333\n",
            "Epoch 7/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.2308 - acc: 0.3333\n",
            "Epoch 8/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.2301 - acc: 0.3333\n",
            "Epoch 9/550\n",
            "120/120 [==============================] - 0s 175us/sample - loss: 0.2295 - acc: 0.3333\n",
            "Epoch 10/550\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.2288 - acc: 0.3333\n",
            "Epoch 11/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.2281 - acc: 0.3333\n",
            "Epoch 12/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.2274 - acc: 0.3333\n",
            "Epoch 13/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.2268 - acc: 0.3333\n",
            "Epoch 14/550\n",
            "120/120 [==============================] - 0s 114us/sample - loss: 0.2261 - acc: 0.3333\n",
            "Epoch 15/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.2253 - acc: 0.3333\n",
            "Epoch 16/550\n",
            "120/120 [==============================] - 0s 106us/sample - loss: 0.2247 - acc: 0.3333\n",
            "Epoch 17/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.2240 - acc: 0.3333\n",
            "Epoch 18/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.2233 - acc: 0.3333\n",
            "Epoch 19/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.2225 - acc: 0.3333\n",
            "Epoch 20/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.2218 - acc: 0.3333\n",
            "Epoch 21/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.2210 - acc: 0.3333\n",
            "Epoch 22/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.2202 - acc: 0.3333\n",
            "Epoch 23/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.2193 - acc: 0.3333\n",
            "Epoch 24/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.2184 - acc: 0.3333\n",
            "Epoch 25/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.2174 - acc: 0.3333\n",
            "Epoch 26/550\n",
            "120/120 [==============================] - 0s 186us/sample - loss: 0.2164 - acc: 0.3333\n",
            "Epoch 27/550\n",
            "120/120 [==============================] - 0s 163us/sample - loss: 0.2154 - acc: 0.3333\n",
            "Epoch 28/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.2143 - acc: 0.3333\n",
            "Epoch 29/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.2131 - acc: 0.3333\n",
            "Epoch 30/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.2118 - acc: 0.3333\n",
            "Epoch 31/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.2105 - acc: 0.3333\n",
            "Epoch 32/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.2090 - acc: 0.3333\n",
            "Epoch 33/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.2076 - acc: 0.3333\n",
            "Epoch 34/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.2060 - acc: 0.3333\n",
            "Epoch 35/550\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.2044 - acc: 0.3333\n",
            "Epoch 36/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.2026 - acc: 0.3750\n",
            "Epoch 37/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.2009 - acc: 0.4167\n",
            "Epoch 38/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1990 - acc: 0.5417\n",
            "Epoch 39/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.1972 - acc: 0.6583\n",
            "Epoch 40/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1953 - acc: 0.6667\n",
            "Epoch 41/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1934 - acc: 0.6667\n",
            "Epoch 42/550\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.1916 - acc: 0.6667\n",
            "Epoch 43/550\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.1898 - acc: 0.6667\n",
            "Epoch 44/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1881 - acc: 0.6667\n",
            "Epoch 45/550\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.1864 - acc: 0.6667\n",
            "Epoch 46/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.1847 - acc: 0.6667\n",
            "Epoch 47/550\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.1832 - acc: 0.6667\n",
            "Epoch 48/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1817 - acc: 0.6667\n",
            "Epoch 49/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.1803 - acc: 0.6667\n",
            "Epoch 50/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.1790 - acc: 0.6667\n",
            "Epoch 51/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1777 - acc: 0.6667\n",
            "Epoch 52/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.1765 - acc: 0.6667\n",
            "Epoch 53/550\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.1754 - acc: 0.6667\n",
            "Epoch 54/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1742 - acc: 0.6667\n",
            "Epoch 55/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1731 - acc: 0.6667\n",
            "Epoch 56/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.1721 - acc: 0.6667\n",
            "Epoch 57/550\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.1711 - acc: 0.6667\n",
            "Epoch 58/550\n",
            "120/120 [==============================] - 0s 179us/sample - loss: 0.1702 - acc: 0.6667\n",
            "Epoch 59/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.1693 - acc: 0.6667\n",
            "Epoch 60/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.1684 - acc: 0.6667\n",
            "Epoch 61/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.1675 - acc: 0.6667\n",
            "Epoch 62/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.1666 - acc: 0.6667\n",
            "Epoch 63/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1658 - acc: 0.6667\n",
            "Epoch 64/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.1650 - acc: 0.6667\n",
            "Epoch 65/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.1642 - acc: 0.6667\n",
            "Epoch 66/550\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.1634 - acc: 0.6667\n",
            "Epoch 67/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1627 - acc: 0.6667\n",
            "Epoch 68/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1620 - acc: 0.6667\n",
            "Epoch 69/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.1612 - acc: 0.6667\n",
            "Epoch 70/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1605 - acc: 0.6667\n",
            "Epoch 71/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.1598 - acc: 0.6667\n",
            "Epoch 72/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1591 - acc: 0.6667\n",
            "Epoch 73/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.1584 - acc: 0.6667\n",
            "Epoch 74/550\n",
            "120/120 [==============================] - 0s 163us/sample - loss: 0.1578 - acc: 0.6667\n",
            "Epoch 75/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.1571 - acc: 0.6667\n",
            "Epoch 76/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1565 - acc: 0.6667\n",
            "Epoch 77/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.1558 - acc: 0.6667\n",
            "Epoch 78/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1552 - acc: 0.6667\n",
            "Epoch 79/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1546 - acc: 0.6667\n",
            "Epoch 80/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1540 - acc: 0.6667\n",
            "Epoch 81/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1534 - acc: 0.6667\n",
            "Epoch 82/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.1528 - acc: 0.6667\n",
            "Epoch 83/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.1522 - acc: 0.6667\n",
            "Epoch 84/550\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.1517 - acc: 0.6667\n",
            "Epoch 85/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1511 - acc: 0.6667\n",
            "Epoch 86/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.1505 - acc: 0.6667\n",
            "Epoch 87/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1500 - acc: 0.6667\n",
            "Epoch 88/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1495 - acc: 0.6667\n",
            "Epoch 89/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1489 - acc: 0.6667\n",
            "Epoch 90/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1484 - acc: 0.6667\n",
            "Epoch 91/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.1479 - acc: 0.6667\n",
            "Epoch 92/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1474 - acc: 0.6667\n",
            "Epoch 93/550\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.1469 - acc: 0.6667\n",
            "Epoch 94/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1464 - acc: 0.6667\n",
            "Epoch 95/550\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.1459 - acc: 0.6667\n",
            "Epoch 96/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.1455 - acc: 0.6667\n",
            "Epoch 97/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.1450 - acc: 0.6667\n",
            "Epoch 98/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.1445 - acc: 0.6667\n",
            "Epoch 99/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.1441 - acc: 0.6667\n",
            "Epoch 100/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1436 - acc: 0.6667\n",
            "Epoch 101/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1432 - acc: 0.6667\n",
            "Epoch 102/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1427 - acc: 0.6667\n",
            "Epoch 103/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1423 - acc: 0.6667\n",
            "Epoch 104/550\n",
            "120/120 [==============================] - 0s 114us/sample - loss: 0.1419 - acc: 0.6667\n",
            "Epoch 105/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1414 - acc: 0.6667\n",
            "Epoch 106/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.1410 - acc: 0.6667\n",
            "Epoch 107/550\n",
            "120/120 [==============================] - 0s 174us/sample - loss: 0.1406 - acc: 0.6667\n",
            "Epoch 108/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1402 - acc: 0.6667\n",
            "Epoch 109/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1398 - acc: 0.6667\n",
            "Epoch 110/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.1394 - acc: 0.6667\n",
            "Epoch 111/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1390 - acc: 0.6667\n",
            "Epoch 112/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.1386 - acc: 0.6667\n",
            "Epoch 113/550\n",
            "120/120 [==============================] - 0s 144us/sample - loss: 0.1383 - acc: 0.6667\n",
            "Epoch 114/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.1379 - acc: 0.6667\n",
            "Epoch 115/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1375 - acc: 0.6667\n",
            "Epoch 116/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.1372 - acc: 0.6667\n",
            "Epoch 117/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1368 - acc: 0.6667\n",
            "Epoch 118/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.1364 - acc: 0.6667\n",
            "Epoch 119/550\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.1361 - acc: 0.6667\n",
            "Epoch 120/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.1357 - acc: 0.6667\n",
            "Epoch 121/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.1354 - acc: 0.6667\n",
            "Epoch 122/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.1350 - acc: 0.6667\n",
            "Epoch 123/550\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.1347 - acc: 0.6667\n",
            "Epoch 124/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.1344 - acc: 0.6667\n",
            "Epoch 125/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1340 - acc: 0.6667\n",
            "Epoch 126/550\n",
            "120/120 [==============================] - 0s 164us/sample - loss: 0.1337 - acc: 0.6667\n",
            "Epoch 127/550\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.1334 - acc: 0.6667\n",
            "Epoch 128/550\n",
            "120/120 [==============================] - 0s 173us/sample - loss: 0.1331 - acc: 0.6667\n",
            "Epoch 129/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.1327 - acc: 0.6667\n",
            "Epoch 130/550\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.1324 - acc: 0.6667\n",
            "Epoch 131/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.1321 - acc: 0.6667\n",
            "Epoch 132/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.1318 - acc: 0.6667\n",
            "Epoch 133/550\n",
            "120/120 [==============================] - 0s 178us/sample - loss: 0.1315 - acc: 0.6667\n",
            "Epoch 134/550\n",
            "120/120 [==============================] - 0s 163us/sample - loss: 0.1312 - acc: 0.6667\n",
            "Epoch 135/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.1309 - acc: 0.6667\n",
            "Epoch 136/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.1306 - acc: 0.6667\n",
            "Epoch 137/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1303 - acc: 0.6667\n",
            "Epoch 138/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.1301 - acc: 0.6667\n",
            "Epoch 139/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1297 - acc: 0.6667\n",
            "Epoch 140/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.1295 - acc: 0.6667\n",
            "Epoch 141/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.1292 - acc: 0.6667\n",
            "Epoch 142/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.1289 - acc: 0.6667\n",
            "Epoch 143/550\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.1286 - acc: 0.6667\n",
            "Epoch 144/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.1284 - acc: 0.6667\n",
            "Epoch 145/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.1281 - acc: 0.6667\n",
            "Epoch 146/550\n",
            "120/120 [==============================] - 0s 183us/sample - loss: 0.1278 - acc: 0.6667\n",
            "Epoch 147/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.1276 - acc: 0.6667\n",
            "Epoch 148/550\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.1273 - acc: 0.6750\n",
            "Epoch 149/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1270 - acc: 0.6750\n",
            "Epoch 150/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.1268 - acc: 0.6750\n",
            "Epoch 151/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1265 - acc: 0.6833\n",
            "Epoch 152/550\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.1263 - acc: 0.6833\n",
            "Epoch 153/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1260 - acc: 0.6833\n",
            "Epoch 154/550\n",
            "120/120 [==============================] - 0s 206us/sample - loss: 0.1258 - acc: 0.6833\n",
            "Epoch 155/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.1256 - acc: 0.6833\n",
            "Epoch 156/550\n",
            "120/120 [==============================] - 0s 179us/sample - loss: 0.1253 - acc: 0.6833\n",
            "Epoch 157/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.1251 - acc: 0.6833\n",
            "Epoch 158/550\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.1248 - acc: 0.6833\n",
            "Epoch 159/550\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.1246 - acc: 0.6833\n",
            "Epoch 160/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.1244 - acc: 0.6833\n",
            "Epoch 161/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.1241 - acc: 0.6917\n",
            "Epoch 162/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1239 - acc: 0.6917\n",
            "Epoch 163/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1237 - acc: 0.6917\n",
            "Epoch 164/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.1234 - acc: 0.6917\n",
            "Epoch 165/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1232 - acc: 0.6917\n",
            "Epoch 166/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.1230 - acc: 0.6917\n",
            "Epoch 167/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1228 - acc: 0.6917\n",
            "Epoch 168/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1225 - acc: 0.6917\n",
            "Epoch 169/550\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.1223 - acc: 0.6917\n",
            "Epoch 170/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.1221 - acc: 0.6917\n",
            "Epoch 171/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1219 - acc: 0.6917\n",
            "Epoch 172/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.1217 - acc: 0.6917\n",
            "Epoch 173/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1215 - acc: 0.7000\n",
            "Epoch 174/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.1213 - acc: 0.7000\n",
            "Epoch 175/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1210 - acc: 0.7000\n",
            "Epoch 176/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1208 - acc: 0.7000\n",
            "Epoch 177/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.1206 - acc: 0.7083\n",
            "Epoch 178/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1204 - acc: 0.7083\n",
            "Epoch 179/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.1202 - acc: 0.7083\n",
            "Epoch 180/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.1200 - acc: 0.7167\n",
            "Epoch 181/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1198 - acc: 0.7167\n",
            "Epoch 182/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.1196 - acc: 0.7167\n",
            "Epoch 183/550\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.1194 - acc: 0.7167\n",
            "Epoch 184/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1192 - acc: 0.7167\n",
            "Epoch 185/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.1190 - acc: 0.7167\n",
            "Epoch 186/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1188 - acc: 0.7250\n",
            "Epoch 187/550\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.1186 - acc: 0.7417\n",
            "Epoch 188/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1184 - acc: 0.7417\n",
            "Epoch 189/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.1182 - acc: 0.7417\n",
            "Epoch 190/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1180 - acc: 0.7417\n",
            "Epoch 191/550\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.1178 - acc: 0.7417\n",
            "Epoch 192/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.1176 - acc: 0.7417\n",
            "Epoch 193/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.1175 - acc: 0.7583\n",
            "Epoch 194/550\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.1173 - acc: 0.7583\n",
            "Epoch 195/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.1171 - acc: 0.7583\n",
            "Epoch 196/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1169 - acc: 0.7667\n",
            "Epoch 197/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.1167 - acc: 0.7667\n",
            "Epoch 198/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1165 - acc: 0.7667\n",
            "Epoch 199/550\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.1163 - acc: 0.7667\n",
            "Epoch 200/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.1161 - acc: 0.7667\n",
            "Epoch 201/550\n",
            "120/120 [==============================] - 0s 182us/sample - loss: 0.1159 - acc: 0.7667\n",
            "Epoch 202/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.1158 - acc: 0.7750\n",
            "Epoch 203/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1156 - acc: 0.7750\n",
            "Epoch 204/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1154 - acc: 0.7750\n",
            "Epoch 205/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1152 - acc: 0.7667\n",
            "Epoch 206/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.1150 - acc: 0.7667\n",
            "Epoch 207/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.1148 - acc: 0.7750\n",
            "Epoch 208/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1147 - acc: 0.7833\n",
            "Epoch 209/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.1145 - acc: 0.7833\n",
            "Epoch 210/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.1143 - acc: 0.7833\n",
            "Epoch 211/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.1141 - acc: 0.7917\n",
            "Epoch 212/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.1140 - acc: 0.7917\n",
            "Epoch 213/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1138 - acc: 0.7917\n",
            "Epoch 214/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.1136 - acc: 0.7917\n",
            "Epoch 215/550\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.1134 - acc: 0.7917\n",
            "Epoch 216/550\n",
            "120/120 [==============================] - 0s 144us/sample - loss: 0.1133 - acc: 0.8000\n",
            "Epoch 217/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.1131 - acc: 0.8083\n",
            "Epoch 218/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.1129 - acc: 0.8083\n",
            "Epoch 219/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.1127 - acc: 0.8083\n",
            "Epoch 220/550\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.1125 - acc: 0.8083\n",
            "Epoch 221/550\n",
            "120/120 [==============================] - 0s 163us/sample - loss: 0.1124 - acc: 0.8083\n",
            "Epoch 222/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.1122 - acc: 0.8167\n",
            "Epoch 223/550\n",
            "120/120 [==============================] - 0s 170us/sample - loss: 0.1120 - acc: 0.8167\n",
            "Epoch 224/550\n",
            "120/120 [==============================] - 0s 177us/sample - loss: 0.1119 - acc: 0.8167\n",
            "Epoch 225/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.1117 - acc: 0.8250\n",
            "Epoch 226/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.1115 - acc: 0.8250\n",
            "Epoch 227/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.1113 - acc: 0.8250\n",
            "Epoch 228/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.1112 - acc: 0.8250\n",
            "Epoch 229/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1110 - acc: 0.8250\n",
            "Epoch 230/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1108 - acc: 0.8250\n",
            "Epoch 231/550\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.1106 - acc: 0.8250\n",
            "Epoch 232/550\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.1105 - acc: 0.8250\n",
            "Epoch 233/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.1103 - acc: 0.8250\n",
            "Epoch 234/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.1101 - acc: 0.8250\n",
            "Epoch 235/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1099 - acc: 0.8250\n",
            "Epoch 236/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.1098 - acc: 0.8250\n",
            "Epoch 237/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.1096 - acc: 0.8250\n",
            "Epoch 238/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.1094 - acc: 0.8250\n",
            "Epoch 239/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.1092 - acc: 0.8250\n",
            "Epoch 240/550\n",
            "120/120 [==============================] - 0s 175us/sample - loss: 0.1091 - acc: 0.8417\n",
            "Epoch 241/550\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.1089 - acc: 0.8417\n",
            "Epoch 242/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.1087 - acc: 0.8417\n",
            "Epoch 243/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1086 - acc: 0.8500\n",
            "Epoch 244/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.1084 - acc: 0.8583\n",
            "Epoch 245/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1082 - acc: 0.8583\n",
            "Epoch 246/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1080 - acc: 0.8583\n",
            "Epoch 247/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1079 - acc: 0.8667\n",
            "Epoch 248/550\n",
            "120/120 [==============================] - 0s 189us/sample - loss: 0.1077 - acc: 0.8750\n",
            "Epoch 249/550\n",
            "120/120 [==============================] - 0s 191us/sample - loss: 0.1076 - acc: 0.8750\n",
            "Epoch 250/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.1074 - acc: 0.8750\n",
            "Epoch 251/550\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.1072 - acc: 0.8750\n",
            "Epoch 252/550\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.1070 - acc: 0.8750\n",
            "Epoch 253/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.1069 - acc: 0.8750\n",
            "Epoch 254/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.1067 - acc: 0.8833\n",
            "Epoch 255/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1065 - acc: 0.8833\n",
            "Epoch 256/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.1063 - acc: 0.8833\n",
            "Epoch 257/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.1062 - acc: 0.8833\n",
            "Epoch 258/550\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.1060 - acc: 0.8833\n",
            "Epoch 259/550\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.1058 - acc: 0.8833\n",
            "Epoch 260/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.1057 - acc: 0.8833\n",
            "Epoch 261/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.1055 - acc: 0.8833\n",
            "Epoch 262/550\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.1053 - acc: 0.8917\n",
            "Epoch 263/550\n",
            "120/120 [==============================] - 0s 172us/sample - loss: 0.1052 - acc: 0.9167\n",
            "Epoch 264/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.1050 - acc: 0.9167\n",
            "Epoch 265/550\n",
            "120/120 [==============================] - 0s 144us/sample - loss: 0.1048 - acc: 0.9167\n",
            "Epoch 266/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.1046 - acc: 0.9167\n",
            "Epoch 267/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.1045 - acc: 0.9167\n",
            "Epoch 268/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.1043 - acc: 0.9167\n",
            "Epoch 269/550\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.1041 - acc: 0.9167\n",
            "Epoch 270/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.1040 - acc: 0.9167\n",
            "Epoch 271/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.1038 - acc: 0.9167\n",
            "Epoch 272/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.1036 - acc: 0.9250\n",
            "Epoch 273/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.1034 - acc: 0.9250\n",
            "Epoch 274/550\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.1033 - acc: 0.9333\n",
            "Epoch 275/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.1031 - acc: 0.9333\n",
            "Epoch 276/550\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.1029 - acc: 0.9333\n",
            "Epoch 277/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.1027 - acc: 0.9333\n",
            "Epoch 278/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.1026 - acc: 0.9333\n",
            "Epoch 279/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.1024 - acc: 0.9333\n",
            "Epoch 280/550\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.1023 - acc: 0.9333\n",
            "Epoch 281/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.1021 - acc: 0.9333\n",
            "Epoch 282/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.1019 - acc: 0.9333\n",
            "Epoch 283/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1017 - acc: 0.9333\n",
            "Epoch 284/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1015 - acc: 0.9333\n",
            "Epoch 285/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.1014 - acc: 0.9333\n",
            "Epoch 286/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.1012 - acc: 0.9333\n",
            "Epoch 287/550\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.1010 - acc: 0.9500\n",
            "Epoch 288/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.1009 - acc: 0.9500\n",
            "Epoch 289/550\n",
            "120/120 [==============================] - 0s 180us/sample - loss: 0.1007 - acc: 0.9583\n",
            "Epoch 290/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1005 - acc: 0.9583\n",
            "Epoch 291/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.1003 - acc: 0.9500\n",
            "Epoch 292/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.1002 - acc: 0.9583\n",
            "Epoch 293/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.1000 - acc: 0.9583\n",
            "Epoch 294/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0998 - acc: 0.9583\n",
            "Epoch 295/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0996 - acc: 0.9583\n",
            "Epoch 296/550\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.0995 - acc: 0.9583\n",
            "Epoch 297/550\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.0993 - acc: 0.9583\n",
            "Epoch 298/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.0991 - acc: 0.9583\n",
            "Epoch 299/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0990 - acc: 0.9583\n",
            "Epoch 300/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.0988 - acc: 0.9583\n",
            "Epoch 301/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0986 - acc: 0.9583\n",
            "Epoch 302/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.0984 - acc: 0.9583\n",
            "Epoch 303/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0982 - acc: 0.9583\n",
            "Epoch 304/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.0981 - acc: 0.9583\n",
            "Epoch 305/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.0979 - acc: 0.9583\n",
            "Epoch 306/550\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.0977 - acc: 0.9583\n",
            "Epoch 307/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.0976 - acc: 0.9583\n",
            "Epoch 308/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0974 - acc: 0.9583\n",
            "Epoch 309/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.0972 - acc: 0.9583\n",
            "Epoch 310/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0970 - acc: 0.9583\n",
            "Epoch 311/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.0969 - acc: 0.9583\n",
            "Epoch 312/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.0967 - acc: 0.9583\n",
            "Epoch 313/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0965 - acc: 0.9583\n",
            "Epoch 314/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0963 - acc: 0.9583\n",
            "Epoch 315/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0962 - acc: 0.9583\n",
            "Epoch 316/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.0960 - acc: 0.9583\n",
            "Epoch 317/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.0958 - acc: 0.9583\n",
            "Epoch 318/550\n",
            "120/120 [==============================] - 0s 147us/sample - loss: 0.0956 - acc: 0.9583\n",
            "Epoch 319/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0955 - acc: 0.9583\n",
            "Epoch 320/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0953 - acc: 0.9667\n",
            "Epoch 321/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0951 - acc: 0.9583\n",
            "Epoch 322/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0949 - acc: 0.9667\n",
            "Epoch 323/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0948 - acc: 0.9583\n",
            "Epoch 324/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0946 - acc: 0.9667\n",
            "Epoch 325/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0944 - acc: 0.9667\n",
            "Epoch 326/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0942 - acc: 0.9583\n",
            "Epoch 327/550\n",
            "120/120 [==============================] - 0s 154us/sample - loss: 0.0941 - acc: 0.9583\n",
            "Epoch 328/550\n",
            "120/120 [==============================] - 0s 150us/sample - loss: 0.0939 - acc: 0.9667\n",
            "Epoch 329/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.0937 - acc: 0.9583\n",
            "Epoch 330/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0936 - acc: 0.9667\n",
            "Epoch 331/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.0934 - acc: 0.9667\n",
            "Epoch 332/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.0932 - acc: 0.9667\n",
            "Epoch 333/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0930 - acc: 0.9667\n",
            "Epoch 334/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.0929 - acc: 0.9667\n",
            "Epoch 335/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.0927 - acc: 0.9667\n",
            "Epoch 336/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.0925 - acc: 0.9667\n",
            "Epoch 337/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.0923 - acc: 0.9667\n",
            "Epoch 338/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.0921 - acc: 0.9667\n",
            "Epoch 339/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.0920 - acc: 0.9667\n",
            "Epoch 340/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0918 - acc: 0.9667\n",
            "Epoch 341/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.0917 - acc: 0.9667\n",
            "Epoch 342/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.0915 - acc: 0.9667\n",
            "Epoch 343/550\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.0913 - acc: 0.9667\n",
            "Epoch 344/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0911 - acc: 0.9667\n",
            "Epoch 345/550\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.0909 - acc: 0.9667\n",
            "Epoch 346/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.0908 - acc: 0.9667\n",
            "Epoch 347/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0906 - acc: 0.9667\n",
            "Epoch 348/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0904 - acc: 0.9667\n",
            "Epoch 349/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0903 - acc: 0.9667\n",
            "Epoch 350/550\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.0901 - acc: 0.9667\n",
            "Epoch 351/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.0899 - acc: 0.9667\n",
            "Epoch 352/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.0897 - acc: 0.9667\n",
            "Epoch 353/550\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.0895 - acc: 0.9667\n",
            "Epoch 354/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.0894 - acc: 0.9667\n",
            "Epoch 355/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0892 - acc: 0.9667\n",
            "Epoch 356/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.0890 - acc: 0.9667\n",
            "Epoch 357/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.0889 - acc: 0.9667\n",
            "Epoch 358/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.0887 - acc: 0.9667\n",
            "Epoch 359/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.0885 - acc: 0.9667\n",
            "Epoch 360/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.0883 - acc: 0.9667\n",
            "Epoch 361/550\n",
            "120/120 [==============================] - 0s 183us/sample - loss: 0.0882 - acc: 0.9667\n",
            "Epoch 362/550\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.0880 - acc: 0.9667\n",
            "Epoch 363/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.0878 - acc: 0.9667\n",
            "Epoch 364/550\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.0876 - acc: 0.9667\n",
            "Epoch 365/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.0875 - acc: 0.9667\n",
            "Epoch 366/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.0873 - acc: 0.9667\n",
            "Epoch 367/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0871 - acc: 0.9667\n",
            "Epoch 368/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.0869 - acc: 0.9667\n",
            "Epoch 369/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0868 - acc: 0.9667\n",
            "Epoch 370/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.0867 - acc: 0.9667\n",
            "Epoch 371/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0865 - acc: 0.9667\n",
            "Epoch 372/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0863 - acc: 0.9667\n",
            "Epoch 373/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.0861 - acc: 0.9667\n",
            "Epoch 374/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.0859 - acc: 0.9667\n",
            "Epoch 375/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0858 - acc: 0.9667\n",
            "Epoch 376/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0856 - acc: 0.9667\n",
            "Epoch 377/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.0854 - acc: 0.9667\n",
            "Epoch 378/550\n",
            "120/120 [==============================] - 0s 155us/sample - loss: 0.0853 - acc: 0.9667\n",
            "Epoch 379/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.0851 - acc: 0.9667\n",
            "Epoch 380/550\n",
            "120/120 [==============================] - 0s 179us/sample - loss: 0.0849 - acc: 0.9667\n",
            "Epoch 381/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0847 - acc: 0.9667\n",
            "Epoch 382/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.0846 - acc: 0.9667\n",
            "Epoch 383/550\n",
            "120/120 [==============================] - 0s 137us/sample - loss: 0.0844 - acc: 0.9667\n",
            "Epoch 384/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.0843 - acc: 0.9667\n",
            "Epoch 385/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.0841 - acc: 0.9667\n",
            "Epoch 386/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0839 - acc: 0.9667\n",
            "Epoch 387/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0837 - acc: 0.9667\n",
            "Epoch 388/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0836 - acc: 0.9667\n",
            "Epoch 389/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.0834 - acc: 0.9667\n",
            "Epoch 390/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.0832 - acc: 0.9667\n",
            "Epoch 391/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.0831 - acc: 0.9667\n",
            "Epoch 392/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0829 - acc: 0.9667\n",
            "Epoch 393/550\n",
            "120/120 [==============================] - 0s 179us/sample - loss: 0.0827 - acc: 0.9667\n",
            "Epoch 394/550\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.0826 - acc: 0.9667\n",
            "Epoch 395/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.0824 - acc: 0.9667\n",
            "Epoch 396/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.0822 - acc: 0.9667\n",
            "Epoch 397/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.0821 - acc: 0.9667\n",
            "Epoch 398/550\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.0819 - acc: 0.9667\n",
            "Epoch 399/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.0817 - acc: 0.9667\n",
            "Epoch 400/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.0816 - acc: 0.9667\n",
            "Epoch 401/550\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 0.0814 - acc: 0.9667\n",
            "Epoch 402/550\n",
            "120/120 [==============================] - 0s 166us/sample - loss: 0.0813 - acc: 0.9667\n",
            "Epoch 403/550\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.0811 - acc: 0.9667\n",
            "Epoch 404/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.0809 - acc: 0.9667\n",
            "Epoch 405/550\n",
            "120/120 [==============================] - 0s 180us/sample - loss: 0.0807 - acc: 0.9667\n",
            "Epoch 406/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0806 - acc: 0.9667\n",
            "Epoch 407/550\n",
            "120/120 [==============================] - 0s 162us/sample - loss: 0.0804 - acc: 0.9667\n",
            "Epoch 408/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.0802 - acc: 0.9667\n",
            "Epoch 409/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.0801 - acc: 0.9667\n",
            "Epoch 410/550\n",
            "120/120 [==============================] - 0s 153us/sample - loss: 0.0799 - acc: 0.9667\n",
            "Epoch 411/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.0797 - acc: 0.9667\n",
            "Epoch 412/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0796 - acc: 0.9667\n",
            "Epoch 413/550\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.0794 - acc: 0.9667\n",
            "Epoch 414/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.0792 - acc: 0.9667\n",
            "Epoch 415/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.0791 - acc: 0.9667\n",
            "Epoch 416/550\n",
            "120/120 [==============================] - 0s 276us/sample - loss: 0.0789 - acc: 0.9667\n",
            "Epoch 417/550\n",
            "120/120 [==============================] - 0s 144us/sample - loss: 0.0788 - acc: 0.9667\n",
            "Epoch 418/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0786 - acc: 0.9667\n",
            "Epoch 419/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0784 - acc: 0.9667\n",
            "Epoch 420/550\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.0783 - acc: 0.9667\n",
            "Epoch 421/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0781 - acc: 0.9667\n",
            "Epoch 422/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.0780 - acc: 0.9667\n",
            "Epoch 423/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.0778 - acc: 0.9667\n",
            "Epoch 424/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.0776 - acc: 0.9667\n",
            "Epoch 425/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0775 - acc: 0.9667\n",
            "Epoch 426/550\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.0773 - acc: 0.9750\n",
            "Epoch 427/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0771 - acc: 0.9667\n",
            "Epoch 428/550\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.0771 - acc: 0.9667\n",
            "Epoch 429/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0768 - acc: 0.9667\n",
            "Epoch 430/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.0766 - acc: 0.9667\n",
            "Epoch 431/550\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.0765 - acc: 0.9750\n",
            "Epoch 432/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0763 - acc: 0.9667\n",
            "Epoch 433/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0762 - acc: 0.9667\n",
            "Epoch 434/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0760 - acc: 0.9667\n",
            "Epoch 435/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0759 - acc: 0.9667\n",
            "Epoch 436/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0757 - acc: 0.9667\n",
            "Epoch 437/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0755 - acc: 0.9667\n",
            "Epoch 438/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0754 - acc: 0.9667\n",
            "Epoch 439/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0752 - acc: 0.9667\n",
            "Epoch 440/550\n",
            "120/120 [==============================] - 0s 183us/sample - loss: 0.0750 - acc: 0.9667\n",
            "Epoch 441/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.0749 - acc: 0.9667\n",
            "Epoch 442/550\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.0748 - acc: 0.9667\n",
            "Epoch 443/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0746 - acc: 0.9667\n",
            "Epoch 444/550\n",
            "120/120 [==============================] - 0s 144us/sample - loss: 0.0745 - acc: 0.9667\n",
            "Epoch 445/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.0743 - acc: 0.9667\n",
            "Epoch 446/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0741 - acc: 0.9667\n",
            "Epoch 447/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0739 - acc: 0.9750\n",
            "Epoch 448/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0738 - acc: 0.9750\n",
            "Epoch 449/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0736 - acc: 0.9667\n",
            "Epoch 450/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.0735 - acc: 0.9667\n",
            "Epoch 451/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0733 - acc: 0.9750\n",
            "Epoch 452/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0732 - acc: 0.9750\n",
            "Epoch 453/550\n",
            "120/120 [==============================] - 0s 175us/sample - loss: 0.0730 - acc: 0.9750\n",
            "Epoch 454/550\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.0729 - acc: 0.9750\n",
            "Epoch 455/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.0728 - acc: 0.9750\n",
            "Epoch 456/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.0725 - acc: 0.9750\n",
            "Epoch 457/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.0724 - acc: 0.9750\n",
            "Epoch 458/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0722 - acc: 0.9750\n",
            "Epoch 459/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0721 - acc: 0.9750\n",
            "Epoch 460/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0719 - acc: 0.9750\n",
            "Epoch 461/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0718 - acc: 0.9750\n",
            "Epoch 462/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0716 - acc: 0.9750\n",
            "Epoch 463/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0715 - acc: 0.9750\n",
            "Epoch 464/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0713 - acc: 0.9750\n",
            "Epoch 465/550\n",
            "120/120 [==============================] - 0s 158us/sample - loss: 0.0712 - acc: 0.9750\n",
            "Epoch 466/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.0710 - acc: 0.9750\n",
            "Epoch 467/550\n",
            "120/120 [==============================] - 0s 169us/sample - loss: 0.0708 - acc: 0.9750\n",
            "Epoch 468/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.0707 - acc: 0.9750\n",
            "Epoch 469/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0705 - acc: 0.9750\n",
            "Epoch 470/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0704 - acc: 0.9750\n",
            "Epoch 471/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0702 - acc: 0.9750\n",
            "Epoch 472/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.0701 - acc: 0.9750\n",
            "Epoch 473/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.0699 - acc: 0.9750\n",
            "Epoch 474/550\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.0698 - acc: 0.9750\n",
            "Epoch 475/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0696 - acc: 0.9750\n",
            "Epoch 476/550\n",
            "120/120 [==============================] - 0s 160us/sample - loss: 0.0695 - acc: 0.9750\n",
            "Epoch 477/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.0693 - acc: 0.9750\n",
            "Epoch 478/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.0692 - acc: 0.9750\n",
            "Epoch 479/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0690 - acc: 0.9750\n",
            "Epoch 480/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0689 - acc: 0.9750\n",
            "Epoch 481/550\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.0687 - acc: 0.9750\n",
            "Epoch 482/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0686 - acc: 0.9750\n",
            "Epoch 483/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0685 - acc: 0.9750\n",
            "Epoch 484/550\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.0683 - acc: 0.9750\n",
            "Epoch 485/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0682 - acc: 0.9750\n",
            "Epoch 486/550\n",
            "120/120 [==============================] - 0s 148us/sample - loss: 0.0680 - acc: 0.9750\n",
            "Epoch 487/550\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.0678 - acc: 0.9750\n",
            "Epoch 488/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0677 - acc: 0.9750\n",
            "Epoch 489/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.0676 - acc: 0.9750\n",
            "Epoch 490/550\n",
            "120/120 [==============================] - 0s 186us/sample - loss: 0.0674 - acc: 0.9750\n",
            "Epoch 491/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0673 - acc: 0.9750\n",
            "Epoch 492/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0671 - acc: 0.9750\n",
            "Epoch 493/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0670 - acc: 0.9750\n",
            "Epoch 494/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.0668 - acc: 0.9750\n",
            "Epoch 495/550\n",
            "120/120 [==============================] - 0s 163us/sample - loss: 0.0667 - acc: 0.9750\n",
            "Epoch 496/550\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.0665 - acc: 0.9750\n",
            "Epoch 497/550\n",
            "120/120 [==============================] - 0s 161us/sample - loss: 0.0664 - acc: 0.9750\n",
            "Epoch 498/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.0663 - acc: 0.9750\n",
            "Epoch 499/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.0661 - acc: 0.9750\n",
            "Epoch 500/550\n",
            "120/120 [==============================] - 0s 133us/sample - loss: 0.0660 - acc: 0.9750\n",
            "Epoch 501/550\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.0659 - acc: 0.9750\n",
            "Epoch 502/550\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.0657 - acc: 0.9750\n",
            "Epoch 503/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0655 - acc: 0.9750\n",
            "Epoch 504/550\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.0654 - acc: 0.9750\n",
            "Epoch 505/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0652 - acc: 0.9750\n",
            "Epoch 506/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0651 - acc: 0.9750\n",
            "Epoch 507/550\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.0650 - acc: 0.9750\n",
            "Epoch 508/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0649 - acc: 0.9750\n",
            "Epoch 509/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0647 - acc: 0.9750\n",
            "Epoch 510/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.0646 - acc: 0.9750\n",
            "Epoch 511/550\n",
            "120/120 [==============================] - 0s 145us/sample - loss: 0.0644 - acc: 0.9750\n",
            "Epoch 512/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0643 - acc: 0.9750\n",
            "Epoch 513/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0641 - acc: 0.9750\n",
            "Epoch 514/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0640 - acc: 0.9750\n",
            "Epoch 515/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0639 - acc: 0.9750\n",
            "Epoch 516/550\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.0637 - acc: 0.9750\n",
            "Epoch 517/550\n",
            "120/120 [==============================] - 0s 125us/sample - loss: 0.0636 - acc: 0.9750\n",
            "Epoch 518/550\n",
            "120/120 [==============================] - 0s 124us/sample - loss: 0.0634 - acc: 0.9750\n",
            "Epoch 519/550\n",
            "120/120 [==============================] - 0s 141us/sample - loss: 0.0634 - acc: 0.9750\n",
            "Epoch 520/550\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.0632 - acc: 0.9750\n",
            "Epoch 521/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0630 - acc: 0.9750\n",
            "Epoch 522/550\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.0629 - acc: 0.9750\n",
            "Epoch 523/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0628 - acc: 0.9750\n",
            "Epoch 524/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0626 - acc: 0.9750\n",
            "Epoch 525/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0625 - acc: 0.9750\n",
            "Epoch 526/550\n",
            "120/120 [==============================] - 0s 136us/sample - loss: 0.0624 - acc: 0.9750\n",
            "Epoch 527/550\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.0623 - acc: 0.9750\n",
            "Epoch 528/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0621 - acc: 0.9750\n",
            "Epoch 529/550\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.0620 - acc: 0.9750\n",
            "Epoch 530/550\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.0618 - acc: 0.9750\n",
            "Epoch 531/550\n",
            "120/120 [==============================] - 0s 157us/sample - loss: 0.0617 - acc: 0.9750\n",
            "Epoch 532/550\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.0616 - acc: 0.9750\n",
            "Epoch 533/550\n",
            "120/120 [==============================] - 0s 146us/sample - loss: 0.0614 - acc: 0.9750\n",
            "Epoch 534/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.0613 - acc: 0.9750\n",
            "Epoch 535/550\n",
            "120/120 [==============================] - 0s 156us/sample - loss: 0.0612 - acc: 0.9750\n",
            "Epoch 536/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.0610 - acc: 0.9750\n",
            "Epoch 537/550\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.0609 - acc: 0.9750\n",
            "Epoch 538/550\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.0608 - acc: 0.9750\n",
            "Epoch 539/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.0607 - acc: 0.9750\n",
            "Epoch 540/550\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.0605 - acc: 0.9750\n",
            "Epoch 541/550\n",
            "120/120 [==============================] - 0s 138us/sample - loss: 0.0604 - acc: 0.9750\n",
            "Epoch 542/550\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.0603 - acc: 0.9750\n",
            "Epoch 543/550\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.0601 - acc: 0.9750\n",
            "Epoch 544/550\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.0600 - acc: 0.9750\n",
            "Epoch 545/550\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.0599 - acc: 0.9750\n",
            "Epoch 546/550\n",
            "120/120 [==============================] - 0s 130us/sample - loss: 0.0597 - acc: 0.9750\n",
            "Epoch 547/550\n",
            "120/120 [==============================] - 0s 151us/sample - loss: 0.0596 - acc: 0.9750\n",
            "Epoch 548/550\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.0595 - acc: 0.9750\n",
            "Epoch 549/550\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.0594 - acc: 0.9750\n",
            "Epoch 550/550\n",
            "120/120 [==============================] - 0s 187us/sample - loss: 0.0593 - acc: 0.9750\n",
            "Prediction :\n",
            "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 2 2 2]\n",
            "Target :\n",
            "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
            "Accuracy: \n",
            "93.33333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKwKufyy6_AB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}